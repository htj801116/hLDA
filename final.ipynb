{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hierarchical Topic Models and the Nested Chinese Restaurant Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### I. Background\n",
    "\n",
    "Recently, complex probabilistic models are increasingly prevalent in various of domains. However, there are several challenges that should be dealt with due to their open-ended nature. That is, the data sets often grow over time, as they growing, they bring new entities and new structures to the fore. Take the problem of learning a topic hierarchy from data for example. Given a collection of __*documents*__, each of which contains a set of __*words*__ and the goal is to discover common usage patterns or __*topics*__ in the documents, and to organize these topics into a hierarchy.\n",
    "\n",
    "This paper proposes a new method that specified a generative probabilistic model for hierarchical structures and adopt Bayesian perspective to learn such structures from data. The hierarchies in this case could be considered as random variables and specified procedurally. In addition, the underlying approach of constructing the probabilistic object is __Chinese restaurant process (CRP)__, a distribution on partitions of integers. In this paper, they extend CRP to a hierarchy of partitions, known as __nested Chinese restaruant process (nCRP)__, and apply it as a representation of prior and posterior distributions for topic hierarchies. To be more specific, each node in the hierarchy is associated with a topic, where a topic is a distribution across words. A document is generated by choosing a path from the root to a leaf, repeatedly sampling topics along that path, and sampling the words from the selected topics. Thus the orga- nization of topics into a hierarchy aims to capture the breadth of usage of topics across the corpus, reflecting underlying syntactic and semantic notions of generality and specificity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### II. Algorithm Description\n",
    "\n",
    "#### A. Chinese Restaurant Process\n",
    "\n",
    "CRP is an analogous to seating customers at tables in a Chinese restaurant. Imagine there is a Chinese restaurant with an infinite number of circular tables, each with infinite capacity. Customer 1 sits at the first table. The next customer either sits at the same table as customer 1, or the next table. The $m$th subsequent customer sits at a table drawn from the following distribution:\n",
    "\n",
    "$$p(\\text{occupied table } i \\text{ | previous customers}) =  \\frac{m_i}{\\gamma+m-1}$$\n",
    "\n",
    "\n",
    "$$p(\\text{next unoccupied table | previous customers}) = \\frac{\\gamma}{\\gamma + m -1}$$\n",
    "\n",
    "where $m_i$ is the number of previous customers at table $i$, and $\\gamma$ is a parameter. After $M$\n",
    "customers sit down, the seating plan gives a partition of $M$ items. This distribution gives\n",
    "the same partition structure as draws from a Dirichlet process.\n",
    "\n",
    "#### B. Nested Chinese Restaurant Process\n",
    "\n",
    "A nested Chinese restaurant process (nCRP) is an extended version of CRP. Suppose that there are an infinite number of infinite-table Chinese restaurants in a city. A restaurant is determined to be the root restaurant and on each of its infinite tables is a card with the name of another restaurant. On each of the tables in those restaurants are cards that refer to other restaurants, and this structure repeats infinitely. Each restaurant is referred to exactly once.  As a result, the whole process could be imagined as an infinitely-branched tree.\n",
    "\n",
    "Now, consider a tourist arrives in the city for a culinary vacation. On the first first day, he select a root Chinese restaurant and selects a table from the equation above. On the second day, he enters to the restaurant refered by previous restaurant , again from the above equation. This process was repeated for $L$ days, and at the end, the tourist has sat at L restaurants which constitute a path from the root to a restaurant at the $L$th level in the infinite tree. After M tourists take L-day vacations, the collection of paths describe a particular L-level subtree of the infinite tree.\n",
    "\n",
    "#### C. Hierarchical Topic Model (hLDA)\n",
    "\n",
    "The hierarchical latent Dirichlet allocation model (hLDA) together with nested Chinese restaruant process (nCRP) illustrate the pattern of words from the collection of documents. There are 3 procedures in hLDA: (1) Draw a path from root-node to a leaf; (2) Select a specific path, draw a vector of topic along the path; (3) Draw the words from the topic. In addition, all documents share the topic associated with the root restaurant.\n",
    "\n",
    "1. Let c1 be the root restaurant.\n",
    "+ For each level $\\ell\\in\\{2,...,L\\}$:\n",
    "    1. Draw a table from restaurant $c_{\\ell-1}$ using CRP. Set $c_{\\ell}$ to be the restaurant reffered to by that table.\n",
    "+ Draw an L-dimensional topic proportion vector $\\theta$ from Dir($\\alpha$).\n",
    "+ For each word $n\\in\\{1,...,N\\}$:\n",
    "    1. Draw $z\\in\\{1,...,L\\}$ from Mult($\\theta$).\n",
    "    + Draw $w_n$ from the topic associated with restaurant $c_z$.\n",
    "\n",
    "<img src=\"hLDA.png\" style=\"width:400px\">\n",
    "\n",
    "* Notation:\n",
    "    * $T$ : L-level infinite-tree - drawn from CRP($\\gamma$)\n",
    "    * $\\theta$ : L-dimensional topic propotional distribution - drawn from Dir($\\alpha$)\n",
    "    * $\\beta$ : probability of words for each topic - drawn from $\\eta$\n",
    "    * $c_{\\ell}$ : L-level paths, given $T$\n",
    "    * $z$ : actual number of topics for each level - drawn from Mult($\\theta$)\n",
    "    * $w$ : word distribution for each topic at each level\n",
    "    * $N$ : number of words - $n\\in\\{1,...,N\\}$\n",
    "    * $M$ : number of documents - $m\\in\\{1,...,M\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### III. Approximate Inference by Gibbs Sampling\n",
    "\n",
    "Gibbs sampling will sample from the posterior nCRP and corresponding topics in the hLDA model. The sampler are divided into 2 parts -- $z_{m,n}$ and $ c_{m,\\ell}$. In addition, variables $\\theta$ and $\\beta$ are integrated out.\n",
    "\n",
    "#### A. Notation\n",
    "\n",
    "* $w_{m,n}$ : the $n$th word in the $m$th documnt\n",
    "* $c_{m,\\ell}$ : the restaurant corresponding to the $\\ell$th topic in document $m$\n",
    "* $z_{m,n}$ : the assignment of the $n$th word in the $m$th document to one of the $L$ available topics\n",
    "\n",
    "#### B. Topic distribution : $z_{m,n}$\n",
    "\n",
    "$$p(z_{i}=j\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf z}_{-i},{\\bf w})\\propto\\frac{n_{-i,j}^{(w_{i})}+\\beta}{n_{-i,j}^{(\\cdot)}+W\\beta}\\frac{n_{-i,j}^{(d_{i})}+\\alpha}{n_{-i,\\cdot}^{(d_{i})}+T\\alpha}$$\n",
    "\n",
    "* $z_{i}$ : assignments of words to topics\n",
    "* $n_{-i,j}^{(w_{i})}$ : number of words assigned to topic $j$ that are the same as $w_i$\n",
    "* $n_{-i,j}^{(\\cdot)}$ : total number of words assigned to topic $j$\n",
    "* $n_{-i,j}^{(d_{i})}$ : number of words from document $d_i$ assigned to topic $j$\n",
    "* $n_{-i,\\cdot}^{(d_{i})}$ : total number of words in document $d_i$\n",
    "* $W$ : number of words have been assigned\n",
    "\n",
    "#### C. Path : ${\\bf c}_{m}$\n",
    "\n",
    "$$p({\\bf c}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf w}, {\\bf c}_{-m}, {\\bf z})\\propto p({\\bf w}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf c}, {\\bf w}_{-m}, {\\bf z})\\cdot p({\\bf c}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf c}_{-m})$$\n",
    "\n",
    "* $p({\\bf c}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf w}, {\\bf c}_{-m}, {\\bf z})$ : posterior of the set of probabilities of possible novel paths\n",
    "* $p({\\bf w}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf c}, {\\bf w}_{-m}, {\\bf z})$ : likelihood of the data given a particular choice of ${\\bf c}_{m}$\n",
    "* $p({\\bf c}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf c}_{-m})$ : prior on ${\\bf c}_{m}$ which implies by the nCRP\n",
    "\n",
    "\n",
    "$$p({\\bf w}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf c}, {\\bf w}_{-m}, {\\bf z})=\\prod\\limits_{\\ell=1}^{L}\\left(\\frac{\\Gamma(n_{c_{m,\\ell},-m}^{(\\cdot)}+W\\eta)}{\\prod_{w}\\Gamma(n_{c_{m,\\ell},-m}^{(w)}+\\eta)}\\frac{\\prod_{w}\\Gamma(n_{c_{m,\\ell},-m}^{(w)}+n_{c_{m,\\ell},m}^{(w)}+\\eta)}{\\Gamma(n_{c_{m,\\ell},-m}^{(\\cdot)}+n_{c_{m,\\ell},m}^{(\\cdot)}+W\\eta)}\\right)$$\n",
    "\n",
    "* $p({\\bf w}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf c}, {\\bf w}_{-m}, {\\bf z})$ : joint distribution of likelihood\n",
    "* $n_{c_{m,\\ell},-m}^{(w)}$ : number of instances of word $w$ that have been assigned to the topic indexed by $c_{m,\\ell}$, not in the document $m$\n",
    "* $W$ : total vocabulary size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### IV. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Package import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import gammaln\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Function construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B.1 Chinese Restaurant Process (CRP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CRP(topic, phi):\n",
    "    '''\n",
    "    CRP gives the probability of topic assignment for specific vocabulary\n",
    "    Return a 1 * j array, where j is the number of topic\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    topic: a list of lists, contains assigned words in each sublist (topic)\n",
    "    phi: double, parameter for CRP\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    p_crp: the probability of topic assignments for new word\n",
    "    '''\n",
    "    p_crp = np.empty(len(topic)+1)\n",
    "    m = sum([len(x) for x in topic])\n",
    "    cm[0] = phi / (phi + m)\n",
    "    for i, word in enumerate(topic):\n",
    "        p_crp[i+1] = len(word) / (phi + m)\n",
    "    return p_crp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B.2 Node Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def node_sampling(corpus_s, phi):\n",
    "    '''\n",
    "    Node sampling samples the number of topics, L\n",
    "    return a j-layer list of lists, where j is the number of topics\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    corpus_s: a list of lists, contains words in each sublist (document)\n",
    "    phi: double, parameter for CRP\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    topic: a list of lists, contains assigned words in each sublist (topic)\n",
    "    '''\n",
    "    topic = []    \n",
    "    for corpus in corpus_s:\n",
    "        for word in corpus:\n",
    "            cm = CRP(topic, phi)\n",
    "            theta = np.random.multinomial(1, (cm/sum(cm))).argmax()\n",
    "            if theta == 0:\n",
    "                topic.append([word])\n",
    "            else:\n",
    "                topic[theta-1].append(word)\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B.3 Gibbs sampling -- $z_{m,n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Z(corpus_s, topic, alpha, beta):\n",
    "    '''\n",
    "    Z samples from LDA model\n",
    "    Return two j-layer list of lists, where j is the number of topics\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    corpus_s: a list of lists, contains words in each sublist (document)\n",
    "    topic: a L-dimensional list of lists, sample from node_sampling\n",
    "    alpha: double, parameter\n",
    "    beta: double, parameter\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    z_topic: a j-dimensional list of lists, drawn from L-dimensioanl topic, j<L\n",
    "    z_doc: a j-dimensioanl list of lists, report from which document the word is assigned to each topic\n",
    "    '''\n",
    "    n_vocab = sum([len(x) for x in corpus_s])\n",
    "    t_zm = np.zeros(n_vocab).astype('int')\n",
    "    z_topic = [[] for _ in topic]\n",
    "    z_doc = [[] for _ in topic]\n",
    "    z_tmp = np.zeros((n_vocab, len(topic)))\n",
    "    assigned = np.zeros((len(corpus_s), len(topic)))\n",
    "    n = 0\n",
    "    for i in range(len(corpus_s)):\n",
    "        for d in range(len(corpus_s[i])): \n",
    "            wi = corpus_s[i][d]   \n",
    "            for j in range(len(topic)):\n",
    "                lik = (z_topic[j].count(wi) + beta) / (assigned[i, j] + n_vocab * beta)\n",
    "                pri = (len(z_topic[j]) + alpha) / ((len(corpus_s[i]) - 1) + len(topic) * alpha)\n",
    "                z_tmp[n, j] = lik * pri\n",
    "                t_zm[n] = np.random.multinomial(1, (z_tmp[n,:]/sum(z_tmp[n,:]))).argmax()\n",
    "            z_topic[t_zm[n]].append(wi)\n",
    "            z_doc[t_zm[n]].append(i)\n",
    "            assigned[i, t_zm[n]] += 1\n",
    "            n += 1\n",
    "    z_topic = [x for x in z_topic if x != []]\n",
    "    z_doc = [x for x in z_doc if x != []]\n",
    "    return z_topic, z_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B.4 Gibbs sampling -- ${\\bf c}_{m}$, CRP prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CRP_prior(corpus_s, doc, phi):\n",
    "    '''\n",
    "    CRP_prior implies by nCRP\n",
    "    Return a m*j array, whre m is the number of documents and j is the number of topics\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    corpus_s: a list of lists, contains words in each sublist (document)\n",
    "    doc: a j-dimensioanl list of lists, drawn from Z function (z_doc)\n",
    "    phi: double, parameter for CRP\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    c_p: a m*j array, for each document the probability of the topics\n",
    "    '''\n",
    "    c_p = np.empty((len(corpus_s), len(doc)))\n",
    "    for i, corpus in enumerate(corpus_s):\n",
    "        p_topic = [[x for x in doc[j] if x != i] for j in range(len(doc))]\n",
    "        tmp = CRP(p_topic, phi)\n",
    "        c_p[i,:] = tmp[1:]\n",
    "    return c_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B.5 Gibbs sampling -- ${\\bf c}_{m}$, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def likelihood(corpus_s, topic, eta):\n",
    "    '''\n",
    "    likelihood gives the propability of data given a particular choice of c\n",
    "    Return a m*j array, whre m is the number of documents and j is the number of topics\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    corpus_s: a list of lists, contains words in each sublist (document)\n",
    "    topic: a j-dimensional list of lists, drawn from Z function (z_assigned)\n",
    "    eta: double, parameter\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    w_m: a m*j array\n",
    "    '''\n",
    "    w_m = np.empty((len(corpus_s), len(topic)))\n",
    "    allword_topic = [word  for t in topic for word in t]\n",
    "    n_vocab = sum([len(x) for x in corpus_s])\n",
    "    for i, corpus in enumerate(corpus_s):\n",
    "        prob_result = []\n",
    "        for j in range(len(topic)):\n",
    "            current_topic = topic[j]\n",
    "            n_word_topic = len(current_topic)\n",
    "            prev_dominator = 1\n",
    "            later_numerator = 1\n",
    "            prob_word = 1  \n",
    "\n",
    "            overlap = [val for val in set(corpus) if val in current_topic]\n",
    "            \n",
    "            prev_numerator = gammaln(len(current_topic) - len(overlap) + n_vocab * 1)\n",
    "            later_dominator = gammaln(len(current_topic) + n_vocab * 1)\n",
    "            for word in corpus:                \n",
    "                corpus_list = corpus                \n",
    "                if current_topic.count(word) - corpus_list.count(word) < 0 :\n",
    "                    a = 0\n",
    "                else:\n",
    "                    a = current_topic.count(word) - corpus_list.count(word)\n",
    "                \n",
    "                prev_dominator += gammaln(a + 1)\n",
    "                later_numerator += gammaln(current_topic.count(word) + 1)\n",
    "           \n",
    "            prev = prev_numerator - prev_dominator\n",
    "            later = later_numerator - later_dominator\n",
    "            \n",
    "            like = prev + later \n",
    "            w_m[i, j] = exp(like)\n",
    "    return w_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B.6 Gibbs sampling -- ${\\bf c}_{m}$, posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def post(w_m, c_p):\n",
    "    '''\n",
    "    Parameter\n",
    "    ---------\n",
    "    w_m: likelihood, drawn from likelihood function\n",
    "    c_p: prior, drawn from CRP_prior function\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    c_m, a m*j list of lists\n",
    "    '''\n",
    "    c_m = (w_m * c_p) / (w_m * c_p).sum(axis = 1)[:, np.newaxis]\n",
    "    return np.array(c_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B.7 Gibbs sampling -- $w_{n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wn(c_m, corpus_s, topic):\n",
    "    '''\n",
    "    wn return the assignment of words for topics, drawn from multinomial distribution\n",
    "    Return a n*1 array, where n is the total number of word\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    c_m: a m*j list of lists, drawn from post function\n",
    "    corpus_s: a list of lists, contains words in each sublist (document)\n",
    "    topic: a j-dimensional list of lists, drawn from Z function (z_assigned)\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    wn_ass: a n*1 array, report the topic assignment for each word\n",
    "    '''\n",
    "    wn_ass = []\n",
    "    for i, corpus in enumerate(corpus_s):\n",
    "        for word in corpus:\n",
    "            theta = np.random.multinomial(1, c_m[i]).argmax()\n",
    "            wn_ass.append(theta)\n",
    "    return np.array(wn_ass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Gibbs sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C.1 Find most common value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_common = lambda x: Counter(x).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C.2 Gibbs sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gibbs(corpus_s, topic, alpha, beta, phi, eta, ite):\n",
    "    '''\n",
    "    gibbs will return the distribution of words for topics\n",
    "    Return a j-dimensional list of lists, where j is the number of topics\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    corpus_s: a list of lists, contains words in each sublist (document)\n",
    "    topic: a j-dimensional list of lists, drawn from Z function (z_assigned)\n",
    "    alpha: double, parameter for Z function\n",
    "    beta: double, parameter for Z function\n",
    "    phi: double, parameter fro CRP_prior function\n",
    "    eta: double, parameter for w_n function\n",
    "    ite: int, number of iteration\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    wn_topic: a j-dimensional list of lists, the distribution of words for topics\n",
    "    '''\n",
    "    n_vocab = sum([len(x) for x in corpus_s])\n",
    "    gibbs = np.empty((n_vocab, ite)).astype('int')\n",
    "   \n",
    "    for i in range(ite):\n",
    "        z_topic, z_doc = Z(corpus_s, topic, alpha, beta)\n",
    "        c_p = CRP_prior(corpus_s, z_doc, phi)\n",
    "        w_m = likelihood(corpus_s, z_topic, eta)\n",
    "        c_m = post(w_m, c_p)\n",
    "        gibbs[:, i] = wn(c_m, corpus_s, z_topic) \n",
    "    # drop first 1/10 data\n",
    "    gibbs = gibbs[:, int(ite/10):]\n",
    "    theta = [most_common(gibbs[x]) for x in range(n_vocab)]\n",
    "    \n",
    "    n_topic = max(theta)+1\n",
    "    \n",
    "    wn_topic = [[] for _ in range(n_topic)]\n",
    "    wn_doc_topic = [[] for _ in range(n_topic)]\n",
    "\n",
    "    doc = 0\n",
    "    n = 0\n",
    "    for i, corpus_s in enumerate(corpus_s):\n",
    "        if doc == i:\n",
    "            for word in corpus_s:\n",
    "                wn_doc_topic[theta[n]].append(word)\n",
    "                n += 1\n",
    "            for j in range(n_topic):\n",
    "                if wn_doc_topic[j] != []:\n",
    "                    wn_topic[j].append(wn_doc_topic[j])\n",
    "        wn_doc_topic = [[] for _ in range(n_topic)]        \n",
    "        doc += 1\n",
    "    wn_topic = [x for x in wn_topic if x != []]\n",
    "    return wn_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. Topic Modeling with hLDA\n",
    "\n",
    "Gibbs sampling in section __`IV`__ distributes the input __*vocabularies*__ from __*documents*__ in __*corpus*__ to available __*topics*__, which sampled from $L$-dimensional topics. In section __`V`__, an $n$-level tree will be presented by tree plot, which the root-node will be more general and the leaves will be more specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. hLDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hLDA(corpus_s, alpha, beta, phi, eta, ite, level):\n",
    "    '''\n",
    "    hLDA generates an n*1 list of lists, where n is the number of level\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    corpus_s: a list of lists, contains words in each sublist (document)\n",
    "    alpha: double, parameter for Z function\n",
    "    beta: double, parameter for Z function\n",
    "    phi: double, parameter fro CRP_prior function\n",
    "    eta: double, parameter for w_n function\n",
    "    ite: int, number of iteration\n",
    "    level: int, number of level\n",
    "    \n",
    "    Return\n",
    "    hLDA_tree: an n*1 list of lists, each sublist represents a level, the sublist in each level represents a topic\n",
    "    node: an n*1 list of lists, returns how many nodes there are in each level\n",
    "    '''\n",
    "    \n",
    "    topic = node_sampling(corpus_s, phi)\n",
    "    \n",
    "    hLDA_tree = [[] for _ in range(level)]\n",
    "    tmp_tree = []\n",
    "    node = [[] for _ in range(level+1)]\n",
    "    node[0].append(1)\n",
    "    \n",
    "    for i in range(level):\n",
    "        if i == 0:\n",
    "            wn_topic = gibbs(corpus_s, topic, alpha, beta, phi, eta, ite)\n",
    "            topic = set([x for list in wn_topic[0] for x in list])\n",
    "            hLDA_tree[0].append(topic)\n",
    "            tmp_tree.append(wn_topic[1:])\n",
    "            tmp_tree = tmp_tree[0]\n",
    "            node[1].append(len(wn_topic[1:]))\n",
    "        else:\n",
    "            for j in range(sum(node[i])):\n",
    "                if tmp_tree == []:\n",
    "                    break\n",
    "                wn_topic = gibbs(tmp_tree[0], topic, alpha, beta, phi, eta, ite)\n",
    "                topic = set([x for list in wn_topic[0] for x in list])\n",
    "                hLDA_tree[i].append(topic)\n",
    "                tmp_tree.remove(tmp_tree[0])\n",
    "                if wn_topic[1:] != []:\n",
    "                    tmp_tree.extend(wn_topic[1:])\n",
    "                node[i+1].append(len(wn_topic[1:]))\n",
    "        \n",
    "    return hLDA_tree, node[:level]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. hLDA plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def HLDA_plot(hLDA_object, Len = 8, save = False):\n",
    "    \n",
    "    from IPython.display import Image, display\n",
    "    def viewPydot(pdot):\n",
    "        plt = Image(pdot.create_png())\n",
    "        display(plt)\n",
    "\n",
    "    words = hLDA_object[0]\n",
    "    struc = hLDA_object[1]\n",
    "      \n",
    "    graph = pydot.Dot(graph_type='graph')\n",
    "    end_index = [np.insert(np.cumsum(i),0,0) for i in struc]\n",
    "    for level in range(len(struc)-1):\n",
    "\n",
    "        leaf_level = level + 1\n",
    "        leaf_word = words[leaf_level]\n",
    "        leaf_struc = struc[leaf_level]\n",
    "        word = words[level]\n",
    "        end_leaf_index = end_index[leaf_level]\n",
    "\n",
    "        for len_root in range(len(word)):\n",
    "            #print(list(word[len_root]))\n",
    "            root_word = '\\n'.join(str(v) for v in list(word[len_root])[:Len])\n",
    "            leaf_index = leaf_struc[len_root]  \n",
    "            start = end_leaf_index[len_root]\n",
    "            end = end_leaf_index[len_root+1]\n",
    "            lf = leaf_word[start:end]  \n",
    "            for l in lf:\n",
    "                #print(list(l))\n",
    "                leaf_w = '\\n'.join(str(v) for v in list(l)[:Len]) \n",
    "                edge = pydot.Edge(root_word, leaf_w)\n",
    "                graph.add_edge(edge)\n",
    "    if save == True:\n",
    "        graph.write_png('graph.png')\n",
    "    viewPydot(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI. Empirical Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For empirical example, the corpus of documents is generated from [Blei's sample data](https://github.com/blei-lab/lda-c). The documents are splitted by paragraph; that is, each paragraph reprents one document. We take first 5 documents to form the sample corpus used in the hLDA model. To form the corpus, we read the corpus as a large list of lists. The sublists in the nested list represent the documents. The elements in each sublist represent the words in specific document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus(document_path):\n",
    "    punc = ['`', ',', \"'\", '.', '!', '?']\n",
    "    corpus = []\n",
    "    with open(document_path, 'r') as f:\n",
    "        for line in f:\n",
    "            for x in punc:\n",
    "                line = line.replace(x, '')\n",
    "            line = line.strip('\\n')\n",
    "            word = line.split(' ')\n",
    "            corpus.append(word)\n",
    "    return(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII. Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "\n",
    "cimport cython\n",
    "import numpy as np\n",
    "\n",
    "@cython.cdivision\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
